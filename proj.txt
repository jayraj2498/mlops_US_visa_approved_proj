To create the flowcahrt we use website : 

1. first we setup template.py we setup all file structures skeleton which is required for project 

2. what are the package we are using inside the project we will mention inside the requirement.txt  file 
    with the hel of -e we able to install particular packages also we able install us_visa as a package 

3. create python environment  conda create -n visa python=3.11 -y  
    - conda activate visa 

4 . - Data base setup : mongodb atlas 
        here we have mongodb atlas database we setup it to store data 
        visa data --> make connectin in mongodb 
        data ingestion -- > in that it will go to db and fetch connection through database  
        - before doing modular coding we do notebook experiment 

        mongodb -- > we create project 
        after creating project we create cluster 
        creted cluster0 set its name & pass jayraj2498 and also add public ip addres --> allow access from anywhere 
        in term of connecting to cluster0 select the driver option --> select python and select version 3.6 or later 
        also copy your connection string --> 
        mongodb+srv://jayraj2498:jayraj2498@cluster0.a78lv.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0

        first -> we create database --> inside we create collection 
        see mongodb.ipynb file . 



    - logging module 
        - whenever we productionize the code in cloud , if error is encountered so you can download that log file and take action
        - we create the custom logger 
    - exception module (own exception) 
        - to raise exception we make our own exception 

    - utility module 
        - in utilility module the functionality we are using frequnetly in your code 
        - make function use it whenever we require (reuselity) 


5. Completed EDA- FE wrt model training with hyperparameter use it inside modular coding 


----------------------------------------------------------------------------------------------------------------- 
6. Modular coding  

Workflow :-> for which file to change first and last 
    1. constant 
    2. entity 
    3. component 
    4. pipeline  
    5. main file 


    component implimentation 

    1. data Ingestion :- getting data from mongodb 
        inside notebook we have done that ,  but we do it with modular coding 

        With help of connection srting we connect to mongodb from that db we fetch data 

        |-> see the flowchart -> data ingestion.png these flow chart is helps you to understand the flow
        how we ingest data to ingest the data what are the components  we need  
        |
        |-> to get data from mongodb to access_data  to do that we need the helper constant 
            this are the constant variable inside our data ingestion pipeline :
            ex : data ingestion dir , feature store file path , training file path , testing file path ,
            train test split ratio , colection name 
        |
        |-> means after ingesting the  data where it keep the data in the local machine so these are the abouve var we need to pass as constant
            if we want to fech data and we need to keep somewhere we need the folder and that our folder name is data ingestion dir inside it we store the data 

            after that we need to convert our dic to csv file and this csv file it will store inside the feature store file path(folder)

            after  that we do train test split operation we get training set & testing set this thing we keep inside the ingested folder 

        |-> so, one folder is get crated Artifact inside that 

         Artifacts means generated output -->(After runnign the code whatever op is generated  we need to save somewhere )

        |-> Artifacts-> feature store --> usvisa.csv   
        |-> Artifacts-> ingested --> train.csv & test.csv      


----->  1. constant : it is the variable that we change it frequnetly in the constructor whenever we required  

          ex: if in future whenever we need to change db name in future for another proj we have to open the code 
          inside data ingestion there it we have to change it manually so, finding that var is difficult task insted of hardcoded we do 
          instead that we do --> contant file inside we mention var , if we need to change it , we open constant file and here we chnage it & everything reflect inside our code it is in the automated way task 

        for every module we have related constant 
        ex data_ingestion related constant 
        this are the constant variable inside our data ingestion pipeline 

        |-> inside __init__.py it we have to mention variable (this are the common constant):->  data ingestion dir , feature store file path , training file path , testing file path ,train test split ratio , colection name
            - MONGODB_URL_KEY = "MONGODB_URL" <-- we have to export this connection string as our env variable (to maintain privacy)
            - in windows we get op called environment variable -> inside create sys variable ,, with help of operating sys package we load that env varibale 
            - By using Git bash command -> export  MONGODB_URL = " "
        
----->   2. entity 
        |-> inside entity folder we have : 
            - artifact_entity 
            - config_entity 

        *- config_entity :- To run the data ingestion we need some constant this constant if we see at the end it is just as path 
            so , 
            to manupulate this constant we required config entity 

            config entity will do what ?-> it take constant it will maek the proper path & this path will be provided in our data ingestion component and data ingestion is get executed 
            After data ingestion is executed and it will give you 2 things 
            - train.csv 
            - test.csv 

        * if we want  after executing data ingestion our next component is --> Data validation 
        Q . If we want to validated our data what we have to do ? 
        - > we have to load train.csv & test.csv then we check the data where it is in correct format or not (if their is data drift problem or not) 

        Means. the output is comming from 1st componet(data_ingestion) train.csv and test.csv  it is input for second component(data_validation) 
        this is our we called it as artifact entity  i.e means we are getting some entity  we are getting some ouptput & this op we are sending next to data_validation and these  op we send to our data_transformation as input then model trainer -> model evaluation -> model pusher 
        
        so, this is the entire pipeline and op we are geting we called it as artifact  (artifact entity) 
        code:: config entity 



        *- Artifact_entity :- After doing data ingestion Artifact will generate --> ingested & feature store train.csv and test.csv  
            - 

        Next task thing to make the connection through monogodb 

        Inside configuration make the file called mongo_db_connection.py

        |-> configuration
            - mongo_db_connection.py :- here we use same code we have written in jupiter notebook 

            Q . why we make a connection in a seprate file configration  ? 
            -> bcoz , whenever we make a connection with mongodb inside data_ingestion, we need to write this much code to inside it so, it will be messy so, we create diff file ans inside we write code in it 

            
        - --> next , our data is in the dictionay form converted to it into dataframe 
             for that we created another class in the diff folder inside that we write logic to convert the data fom dic to df 
        us_visa 
        |-> __init__.py 
        |-> usvisa_data.py   <-- inside we write the logic 


     

----->    3. component :
            Inside component we intialize al thing we have done abouve 
            we wriet class DataIngestion where we inyiallize DataIngestionConfig() 
            then we reed data as dataframe and saving them in side the path by using funtin def export_data_into_feature_store(self)->DataFrame:
            the we do train , test , split opration  def split_data_as_train_test(self,dataframe:DataFrame) -> None : and writening  train_set.to_csv and  test_set.to_csv to path 
            then we intiiate  def initiate_data_ingestion(self) ->DataIngestionArtifact: we which return a DataIngestionArtifact for next component as input 


        4. pipeline 
        we have 2 pipe training pipeline & prediction pipeline 
        here now we use training pipline   

        we intiallize all method inside that we pefoem inside data_ingestion 
        inside trainign pipline we use def initiate_data_ingestion(self) -> DataIngestionArtifact : which return artifact 
                
        @dataclass
        class DataIngestionArtifact:
            trained_file_path:str 
            test_file_path:str  

        this return trained_file_path and test_file_path  so, inside pipline we intiate one method and it will  automated all thing 

        At the end  inside we we make a funtion called   run_pipeline(self, ) -> None: 
        which is responsible for excecting  " all component pipline "  task in one way    
                






        5. main file 





final op : 
-> Artifact folder  : inside we have --> Time stamp folder --> inside we have --> Data_ingestion folder 
    ->  time stamp folder 
        ->  Data_ingestion

            -> feature_store 
                - usvisa.csv 
    
            -> ingested 
                - train.csv
                - test.csv 
    